"""
Evaluating LLM agent outputs using a custom prompt and rubric.

Customize:
- Model
- Reference context (e.g. standards or policies)
- Input/output examples
- Evaluation prompt and scoring rubric
- Output schema for judge
"""

import os
import json
from openevals.llm import create_llm_as_judge
from typing_extensions import TypedDict

# Define expected format for judge result
class ValidationJudgeResult(TypedDict):
    score: float
    explanation: str

# Set your API key for the selected LLM provider 
os.environ["ANTHROPIC_API_KEY"] = "sk-your-anthropic-key"

# Load standard or reference document
with open("path/to/your-standard.json", "r", encoding="utf-8") as f:
    standard_context = json.load(f)

# Prompt used to instruct the judge model how to evaluate the output
# It includes task instructions, agent expectations, and a rubric
VALIDATION_AGENT_JUDGE_PROMPT = """
You are an evaluator for a Standards Compliance Agent.

The agent receives:
- A reported non-conformance finding
- A cited clause number
- A full standard document with each clause keyed by its 'Designator' number

The agent must:
1. Extract core obligations and evidence from the finding
2. Match the best clause from the standard context
3. Validate the structure of the finding:
   - Obligations should begin with "must"/"shall"
   - The language should clearly express a requirement

The agent outputs structured JSON with:
- is_valid: boolean
- updated_finding: optionally reworded version of the input
- updated_clause_no: best-matching clause number
- explanation: justification for the decision

<context>
{context}
</context>

<input>
{inputs}
</input>

<output>
{outputs}
</output>

<Rubric>
Score the output:
- 1.0: Best clause selected, structure correct, output valid and well-formed
- 0.5: Mostly correct with minor issues
- 0.0: Incorrect clause or invalid structure or output
"""

# Sample input provided to the agent 
inputs = {
    "clause_no": "6.2.5",
    "finding": "The laboratory must ensure staff responsible for packaging and transport of pathology specimens receive IATA training."
}

# Sample output generated by the agent 
outputs = {
    "is_valid": { "value": True },
    "updated_finding": {
        "value": "The laboratory must ensure staff responsible for packaging and transport of pathology specimens receive IATA training."
    },
    "updated_clause_no": { "value": "6.2.5" },
    "explanation": {
        "value": "Finding clearly aligns with clause 6.2.5 and is structurally valid with 'must' at the beginning."
    }
}

# Instantiate the LLM-based judge using your custom prompt and schema
llm_as_judge = create_llm_as_judge(
    prompt=VALIDATION_AGENT_JUDGE_PROMPT,
    choices=[0.0, 0.5, 1.0],
    model="anthropic:claude-3-5-sonnet-latest",
    output_schema=ValidationJudgeResult
)

# Run the evaluation 
eval_result = llm_as_judge(
    inputs=inputs,
    outputs=outputs,
    context=standard_context
)

print("=== Evaluation Result ===")
print(json.dumps(eval_result))
